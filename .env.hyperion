# Hyper - Native macOS Configuration
# Source this file before running: source .env.hyper

ENABLE_FILE_WATCHER="false"
INDEX_SOURCE_PATH="/Users/maxmednikov/MaxSpace/dev-squad"
# ========================================
# APPLICATION
# ========================================
HTTP_PORT="7095"
LOG_LEVEL="info"

# Claude agents directory (for import feature)
CLAUDE_AGENTS_DIR="/Users/maxmednikov/MaxSpace/dev-squad/.claude/agents"

# ========================================
# MONGODB (Cloud - Atlas)
# ========================================
MONGODB_URI="mongodb+srv://dev:fvOKzv9enD8CSVwD@devdb.yqf8f8r.mongodb.net/?retryWrites=true&w=majority&appName=devDB"
MONGODB_DATABASE="megha_hyper_coordinator_db_dev_squad"

# ========================================
# QDRANT (Cloud)
# ========================================
QDRANT_URL="https://2445de59-e696-4952-8934-a89c7c7cfec0.us-east4-0.gcp.cloud.qdrant.io:6333"
QDRANT_API_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.F3aziDpD7sb7LVz6JJ_XcteFauRGHCtvOz5hbR7JYho"
# Code index collection name (use different collections for different embedding models)
QDRANT_CODE_COLLECTION="hyper_dev_squad_code"  
QDRANT_KNOWLEDGE_COLLECTION="hyper_dev_squad_knowledge"  


# ========================================
# EMBEDDINGS
# ========================================
# Using Ollama (GPU-accelerated, runs locally)
EMBEDDING="ollama"
OLLAMA_URL="http://localhost:11434"
OLLAMA_MODEL="nomic-embed-text"

# Alternative: Voyage AI (Cloud - Anthropic recommended)
# EMBEDDING="voyage"
# VOYAGE_API_KEY="pa-_qm9GC-XD8SfJ2lqDonMb0yDL3by9hWt2Hp9hM_Dzq-"
# VOYAGE_MODEL="voyage-3"

# Available Ollama models (dimensions auto-detected):
#   nomic-embed-text  - 768 dimensions (default, fast, good quality)
#   all-minilm        - 384 dimensions (smallest, fastest)
#   embeddinggemma    - 768 dimensions (Google Gemma-based, 307M params)



# ========================================
# CODE INDEXING (Optional)
# ========================================
# CODE_INDEX_FOLDERS="/Users/maxmednikov/MaxSpace/dev-squad"
# CODE_INDEX_AUTO_SCAN="false"
CODE_INDEX_AUTO_RECREATE="true"

# ========================================
# AI CONFIGURATION (for Chat)
# ========================================
# AI provider: "openai" (supports Ollama via OpenAI-compatible API)
AI_PROVIDER="openai"

# OPENAI_BASE_URL="https://api.groq.com/openai/v1"
# OPENAI_API_KEY="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"
# AI_MODEL="openai/gpt-oss-120b"


# Ollama Configuration (OpenAI-compatible endpoint)
OPENAI_BASE_URL="http://localhost:11434/v1"
OPENAI_API_KEY="ollama"  # Ollama doesn't require real key, but library needs one
AI_MODEL="gpt-oss:120b-cloud"
# AI_MODEL="granite4:small-h"
# AI_MODEL="kimi-k2:1t-cloud"





# Generation settings
MAX_ITERATIONS=1000        # Maximum agentic reasoning iterations
MAX_TOOL_CALLS=500         # Maximum tool calls per chat session
MAX_OUT_TOKENS=14096       # Maximum output tokens
TEMPERATURE=0.2           # Model temperature (0.0-1.0)

# Optional: JWT Authentication (leave commented for local dev)
# JWT_SECRET=your-secret-key-here